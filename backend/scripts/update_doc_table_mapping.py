#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ–‡æª”è¡¨æ ¼æ˜ å°„ç”Ÿæˆè…³æœ¬

ç‚ºæ‰€æœ‰ 26 å€‹ .doc/.docx æ–‡ä»¶ç”Ÿæˆå®Œæ•´çš„è¡¨æ ¼æ˜ å°„é…ç½®ï¼Œ
åŒ…æ‹¬æ¯å€‹ç¯€é»çš„ table IDsã€è¡Œæ•¸ã€åˆ—ä¿¡æ¯ç­‰å…ƒæ•¸æ“šã€‚
"""

import json
import logging
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ï¼Œä»¥ä¾¿å°å…¥ parsers å’Œ models
sys.path.insert(0, str(Path(__file__).parent.parent))

from parsers.doc_parser import DocParser
from models import DocumentContent

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ComprehensiveDocTableMapper:
    """å®Œæ•´çš„æ–‡æª”è¡¨æ ¼æ˜ å°„ç”Ÿæˆå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ˜ å°„ç”Ÿæˆå™¨"""
        self.project_root = Path(__file__).parent.parent.parent
        self.doc_dir = self.project_root / "doc"
        self.output_dir = self.project_root / "backend" / "cache" / "doc-tables"
        self.tree_structure_path = self.project_root / "static" / "data" / "tree-structure.json"
        self.mapping_output_path = self.project_root / "static" / "data" / "doc-table-mapping.json"

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–è§£æå™¨
        cache_dir = self.project_root / "backend" / "cache"
        self.parser = DocParser(cache_dir)

        # ç”¨æ–¼å­˜å„²æ˜ å°„çµæœ
        self.doc_table_mapping = {}  # doc_id -> {tables: [...], metadata: {...}}
        self.node_to_doc = {}  # node_id -> doc_id
        self.table_counter = 0

        logger.info(f"ğŸ“ æ–‡æª”ç›®éŒ„: {self.doc_dir}")
        logger.info(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    def process_all_documents(self):
        """è™•ç†æ‰€æœ‰æ–‡æª”æ–‡ä»¶"""
        logger.info("ğŸš€ é–‹å§‹è™•ç†æ‰€æœ‰æ–‡æª”...")

        # åŒæ™‚æŸ¥æ‰¾ .doc å’Œ .docx æ–‡ä»¶
        doc_files = sorted(list(self.doc_dir.glob("*.doc")) + list(self.doc_dir.glob("*.docx")))

        if not doc_files:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• .doc/.docx æ–‡ä»¶")
            return

        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} å€‹æ–‡æª”æ–‡ä»¶")

        successful = 0
        failed = []

        for i, doc_file in enumerate(doc_files, 1):
            try:
                logger.info(f"[{i}/{len(doc_files)}] â¡ï¸  è™•ç†: {doc_file.name}")
                self.process_document(doc_file)
                successful += 1
                logger.info(f"    âœ… æˆåŠŸ")
            except Exception as e:
                logger.error(f"    âŒ å¤±æ•—: {str(e)}")
                failed.append(doc_file.name)

        # ç”Ÿæˆæ˜ å°„æ–‡ä»¶
        logger.info("\nğŸ“‹ ç”Ÿæˆå®Œæ•´æ˜ å°„é…ç½®...")
        self.generate_comprehensive_mapping()

        # æ‘˜è¦
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… è™•ç†å®Œæˆ!")
        logger.info(f"   - æˆåŠŸ: {successful} å€‹æ–‡æª”")
        logger.info(f"   - å¤±æ•—: {len(failed)} å€‹æ–‡æª”")
        if failed:
            logger.warning(f"   - å¤±æ•—åˆ—è¡¨: {', '.join(failed)}")
        logger.info(f"   - ç¸½è¡¨æ ¼æ•¸: {self.table_counter}")
        logger.info(f"{'='*60}\n")

    def process_document(self, doc_path: Path):
        """
        è™•ç†å–®å€‹æ–‡æª”æ–‡ä»¶

        Args:
            doc_path: æ–‡æª”è·¯å¾‘
        """
        # è§£ææ–‡æª”
        doc_content = self.parser.parse_file(doc_path)
        if not doc_content:
            raise Exception(f"ç„¡æ³•è§£ææ–‡æª”: {doc_path.name}")

        doc_id = doc_content.doc_id
        table_ids = []
        total_tables = 0
        total_rows = 0

        # æ§‹å»ºè¼¸å‡ºæ•¸æ“šçµæ§‹
        doc_data = {
            "doc_id": doc_id,
            "filename": doc_content.filename,
            "title": doc_content.title,
            "sections": []
        }

        # æå–è¡¨æ ¼æ•¸æ“š
        for section in doc_content.sections:
            if section.tables:
                section_data = {
                    "id": section.id,
                    "heading": section.heading,
                    "level": section.level,
                    "tables": []
                }

                for table_idx, table in enumerate(section.tables):
                    table_id = f"{doc_id}-section-{section.id}-table-{table_idx}"
                    table_ids.append(table_id)
                    total_tables += 1
                    total_rows += len(table.rows)
                    self.table_counter += 1

                    section_data["tables"].append({
                        "id": table_id,
                        "headers": table.headers,
                        "rows": table.rows,
                        "row_count": len(table.rows),
                        "column_count": len(table.headers)
                    })

                doc_data["sections"].append(section_data)

        # å­˜å„²æ˜ å°„ä¿¡æ¯
        self.doc_table_mapping[doc_id] = {
            "filename": doc_content.filename,
            "title": doc_content.title,
            "tables": table_ids,
            "total_tables": total_tables,
            "total_rows": total_rows,
            "sections": len(doc_data["sections"]),
            "metadata": {
                "file_size": doc_path.stat().st_size,
                "parsed_at": datetime.now().isoformat()
            }
        }

        # å¯«å…¥ JSON æ–‡ä»¶
        output_path = self.output_dir / f"{doc_id}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(doc_data, f, ensure_ascii=False, indent=2)

        logger.info(f"    ğŸ“Š è¡¨æ ¼: {total_tables} å€‹, è¡Œæ•¸: {total_rows}")
        logger.info(f"    ğŸ’¾ å·²ä¿å­˜: {output_path.name}")

    def generate_comprehensive_mapping(self):
        """
        ç”Ÿæˆå®Œæ•´çš„æ˜ å°„é…ç½®æ–‡ä»¶

        åŒ…å«ï¼š
        - æ‰€æœ‰æ–‡æª”çš„è¡¨æ ¼æ˜ å°„
        - ç¯€é»åˆ°æ–‡æª”çš„æ˜ å°„
        - è¡¨æ ¼å…ƒæ•¸æ“š
        """
        # è®€å–æ¨¹çµæ§‹
        try:
            with open(self.tree_structure_path, 'r', encoding='utf-8') as f:
                tree = json.load(f)
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•è®€å–æ¨¹çµæ§‹: {str(e)}")
            return

        # æ§‹å»ºç¯€é»åˆ°æ–‡æª”çš„æ˜ å°„
        node_mappings = {}
        self._traverse_tree_for_mapping(tree["root"], node_mappings)

        # ç”Ÿæˆå®Œæ•´çš„æ˜ å°„çµæ§‹
        complete_mapping = {
            "version": "3.0.0",
            "description": "å®Œæ•´çš„æ–‡æª”è¡¨æ ¼æ˜ å°„é…ç½®",
            "generated_at": datetime.now().isoformat(),
            "summary": {
                "total_documents": len(self.doc_table_mapping),
                "total_tables": self.table_counter,
                "node_mappings": len(node_mappings)
            },
            "documents": self.doc_table_mapping,
            "nodes": node_mappings
        }

        # å¯«å…¥æ˜ å°„æ–‡ä»¶
        try:
            with open(self.mapping_output_path, 'w', encoding='utf-8') as f:
                json.dump(complete_mapping, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… å®Œæ•´æ˜ å°„å·²ç”Ÿæˆ: {self.mapping_output_path.name}")
            logger.info(f"   ğŸ“Š æ–‡æª”ç¸½æ•¸: {complete_mapping['summary']['total_documents']}")
            logger.info(f"   ğŸ“Œ ç¯€é»æ˜ å°„: {complete_mapping['summary']['node_mappings']}")
            logger.info(f"   ğŸ“‹ è¡¨æ ¼ç¸½æ•¸: {complete_mapping['summary']['total_tables']}")
        except Exception as e:
            logger.error(f"âŒ å¯«å…¥æ˜ å°„æ–‡ä»¶å¤±æ•—: {str(e)}")

    def _traverse_tree_for_mapping(self, node, node_mappings):
        """
        éæ­¸éæ­·æ¨¹çµæ§‹ï¼Œç”Ÿæˆç¯€é»æ˜ å°„

        Args:
            node: ç•¶å‰ç¯€é»
            node_mappings: ç¯€é»æ˜ å°„å­—å…¸
        """
        node_id = node.get("id")
        node_type = node.get("type")
        node_label = node.get("label", "")

        # å˜—è©¦æ‰¾åˆ°è©²ç¯€é»å°æ‡‰çš„æ–‡æª”
        if "metadata" in node:
            metadata = node.get("metadata", {})
            doc_id = metadata.get("doc_id")

            # æ ¹æ“š doc_id æŸ¥æ‰¾å°æ‡‰çš„æ–‡æª”
            if doc_id and doc_id in self.doc_table_mapping:
                doc_info = self.doc_table_mapping[doc_id]
                node_mappings[node_id] = {
                    "node_type": node_type,
                    "label": node_label,
                    "doc_id": doc_id,
                    "doc_title": doc_info["title"],
                    "tables": doc_info["tables"],
                    "total_tables": doc_info["total_tables"],
                    "total_rows": doc_info["total_rows"],
                    "sections": doc_info["sections"]
                }

                logger.info(f"   âœ“ {node_id}: å·²æ˜ å°„åˆ° {doc_id} ({doc_info['total_tables']} å€‹è¡¨æ ¼)")

        # éæ­¸è™•ç†å­ç¯€é»
        if "children" in node:
            for child in node["children"]:
                self._traverse_tree_for_mapping(child, node_mappings)


def main():
    """ä¸»å‡½æ•¸"""
    try:
        mapper = ComprehensiveDocTableMapper()
        mapper.process_all_documents()
        return 0
    except Exception as e:
        logger.error(f"âŒ è‡´å‘½éŒ¯èª¤: {str(e)}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
