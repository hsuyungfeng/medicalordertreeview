#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æª”è¡¨æ ¼æ‰¹é‡æå–è…³æœ¬

å¾æ‰€æœ‰ .doc æ–‡ä»¶æå–è¡¨æ ¼æ•¸æ“šï¼Œç”Ÿæˆå‰ç«¯å¯ç”¨çš„ JSON æ–‡ä»¶
ä»¥åŠç¯€é»æ˜ å°„é…ç½®ã€‚
"""

import json
import logging
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ï¼Œä»¥ä¾¿å°å…¥ parsers å’Œ models
sys.path.insert(0, str(Path(__file__).parent.parent))

from parsers.doc_parser import DocParser
from models import DocumentContent

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DocTableExporter:
    """æ–‡æª”è¡¨æ ¼å°å‡ºå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å°å‡ºå™¨"""
        self.project_root = Path(__file__).parent.parent.parent
        self.doc_dir = self.project_root / "doc"
        self.output_dir = self.project_root / "backend" / "cache" / "doc-tables"
        self.tree_structure_path = self.project_root / "static" / "data" / "tree-structure.json"
        self.mapping_output_path = self.project_root / "static" / "data" / "doc-table-mapping.json"

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–è§£æå™¨
        cache_dir = self.project_root / "backend" / "cache"
        self.parser = DocParser(cache_dir)

        logger.info(f"ğŸ“ æ–‡æª”ç›®éŒ„: {self.doc_dir}")
        logger.info(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    def export_all(self):
        """æ‰¹é‡å°å‡ºæ‰€æœ‰æ–‡æª”è¡¨æ ¼"""
        logger.info("ğŸš€ é–‹å§‹æ‰¹é‡æå–æ–‡æª”è¡¨æ ¼...")

        # æŸ¥æ‰¾æ‰€æœ‰ .doc æ–‡ä»¶
        doc_files = sorted(self.doc_dir.glob("*.doc"))
        if not doc_files:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• .doc æ–‡ä»¶")
            return {}

        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} å€‹ .doc æ–‡ä»¶")

        exported_docs = {}
        failed_docs = []

        for doc_file in doc_files:
            try:
                logger.info(f"â¡ï¸  è™•ç†: {doc_file.name}")
                doc_data = self.export_document(doc_file)
                if doc_data:
                    exported_docs[doc_data["doc_id"]] = doc_data
                    logger.info(f"âœ… æˆåŠŸæå–: {doc_file.name}")
                else:
                    logger.warning(f"âš ï¸ ç„¡æ³•è§£æ: {doc_file.name}")
                    failed_docs.append(doc_file.name)
            except Exception as e:
                logger.error(f"âŒ è§£æå¤±æ•— {doc_file.name}: {str(e)}")
                failed_docs.append(doc_file.name)

        # ç”Ÿæˆæ˜ å°„æ–‡ä»¶
        logger.info("ğŸ“‹ ç”Ÿæˆç¯€é»æ˜ å°„æ–‡ä»¶...")
        self.generate_mapping(exported_docs)

        # ç¸½çµ
        logger.info(f"\n{'='*50}")
        logger.info(f"âœ… æå–å®Œæˆ!")
        logger.info(f"   - æˆåŠŸ: {len(exported_docs)} å€‹æ–‡æª”")
        logger.info(f"   - å¤±æ•—: {len(failed_docs)} å€‹æ–‡æª”")
        if failed_docs:
            logger.warning(f"   - å¤±æ•—çš„æ–‡æª”: {', '.join(failed_docs)}")
        logger.info(f"{'='*50}\n")

        return exported_docs

    def export_document(self, doc_path: Path) -> dict:
        """
        å°å‡ºå–®å€‹æ–‡æª”çš„è¡¨æ ¼

        Args:
            doc_path: æ–‡æª”è·¯å¾‘

        Returns:
            å°å‡ºçš„æ–‡æª”æ•¸æ“š
        """
        # è§£ææ–‡æª”
        doc_content = self.parser.parse_file(doc_path)
        if not doc_content:
            logger.warning(f"âš ï¸ ç„¡æ³•è§£ææ–‡æª”: {doc_path.name}")
            return None

        # æ§‹å»ºè¼¸å‡ºæ•¸æ“š
        doc_data = {
            "doc_id": doc_content.doc_id,
            "filename": doc_content.filename,
            "title": doc_content.title,
            "sections": []
        }

        # æå–åŒ…å«è¡¨æ ¼çš„ sections
        table_count = 0
        for section in doc_content.sections:
            if section.tables:
                section_data = {
                    "id": section.id,
                    "heading": section.heading,
                    "level": section.level,
                    "tables": [
                        {
                            "headers": t.headers,
                            "rows": t.rows
                        }
                        for t in section.tables
                    ]
                }
                doc_data["sections"].append(section_data)
                table_count += len(section.tables)

        # å¯«å…¥ JSON æ–‡ä»¶
        output_path = self.output_dir / f"{doc_content.doc_id}.json"
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(doc_data, f, ensure_ascii=False, indent=2)
            logger.info(f"   ğŸ“Š æå–è¡¨æ ¼: {table_count} å€‹ â†’ {output_path.name}")
        except Exception as e:
            logger.error(f"   âŒ å¯«å…¥å¤±æ•—: {str(e)}")
            return None

        return doc_data

    def generate_mapping(self, exported_docs: dict):
        """
        ç”Ÿæˆç¯€é»æ˜ å°„æ–‡ä»¶

        Args:
            exported_docs: å·²å°å‡ºçš„æ–‡æª”æ•¸æ“š
        """
        # è®€å–æ¨¹çµæ§‹
        try:
            with open(self.tree_structure_path, 'r', encoding='utf-8') as f:
                tree = json.load(f)
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•è®€å–æ¨¹çµæ§‹: {str(e)}")
            return

        # æ§‹å»ºæ˜ å°„
        mapping = {
            "version": "2.0.0",
            "description": "ç¯€é»åˆ°æ–‡æª”è¡¨æ ¼çš„æ˜ å°„",
            "generated_at": datetime.now().isoformat(),
            "mappings": {}
        }

        # éæ­·æ¨¹çµæ§‹ï¼Œå»ºç«‹æ˜ å°„
        self._traverse_tree(tree["root"], exported_docs, mapping["mappings"])

        # å¯«å…¥æ˜ å°„æ–‡ä»¶
        try:
            with open(self.mapping_output_path, 'w', encoding='utf-8') as f:
                json.dump(mapping, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æ˜ å°„æ–‡ä»¶å·²ç”Ÿæˆ: {self.mapping_output_path.name}")
            logger.info(f"   ğŸ“ åŒ…å« {len(mapping['mappings'])} å€‹ç¯€é»æ˜ å°„")
        except Exception as e:
            logger.error(f"âŒ å¯«å…¥æ˜ å°„æ–‡ä»¶å¤±æ•—: {str(e)}")

    def _traverse_tree(self, node, exported_docs, mappings):
        """
        éæ­¸éæ­·æ¨¹çµæ§‹ï¼Œå»ºç«‹æ˜ å°„

        Args:
            node: ç•¶å‰ç¯€é»
            exported_docs: å·²å°å‡ºçš„æ–‡æª”æ•¸æ“š
            mappings: æ˜ å°„å­—å…¸
        """
        node_id = node.get("id")
        node_label = node.get("label", "")

        # æª¢æŸ¥è©²ç¯€é»æ˜¯å¦æœ‰å°æ‡‰çš„æ–‡æª”
        if "metadata" in node:
            metadata = node["metadata"]
            if "doc_id" in metadata:
                doc_id = metadata["doc_id"]
                # æŸ¥æ‰¾å°æ‡‰çš„ .doc æ–‡ä»¶
                doc_file = list(self.doc_dir.glob(f"{doc_id}*.doc"))
                if doc_file:
                    doc_filename = doc_file[0].stem
                    # æª¢æŸ¥æ˜¯å¦å·²å°å‡º
                    if doc_filename in exported_docs:
                        doc_data = exported_docs[doc_filename]
                        has_tables = len(doc_data["sections"]) > 0

                        mappings[node_id] = {
                            "label": node_label,
                            "doc_id": doc_filename,
                            "has_tables": has_tables,
                            "doc_title": doc_data["title"],
                            "table_count": sum(
                                len(s["tables"]) for s in doc_data["sections"]
                            )
                        }

        # éæ­¸è™•ç†å­ç¯€é»
        if "children" in node:
            for child in node["children"]:
                self._traverse_tree(child, exported_docs, mappings)


def main():
    """ä¸»å‡½æ•¸"""
    try:
        exporter = DocTableExporter()
        exporter.export_all()
        return 0
    except Exception as e:
        logger.error(f"âŒ è‡´å‘½éŒ¯èª¤: {str(e)}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
